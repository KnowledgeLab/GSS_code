{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #########\n",
    "    # small script to amend the varTypes dict that stores variable types\n",
    "    # the original list is in an Excel file in the Data folder on my local machine \n",
    "    #\n",
    "    import pandas as pd\n",
    "    from cPickle import load, dump\n",
    "    \n",
    "    pathToData='../Data/'\n",
    "    temp_file = open(pathToData + 'variableTypes.pickle', 'rb')\n",
    "    varTypes = load(temp_file)\n",
    "    temp_file.close()\n",
    "\n",
    "    df_vartypes = pd.Series(varTypes)\n",
    "    df_vartypes['HOMOSEX'] = 'CL'\n",
    "    df_vartypes['LIFE'] = 'CL'\n",
    "\n",
    "    # temp_file = open(pathToData + 'variableTypes.pickle', 'wb')\n",
    "    dump(df_vartypes.to_dict(), open(pathToData + 'variableTypes.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Apr 02, 2014\n",
    "@author: Misha\n",
    "\n",
    "description:\n",
    "This file contains classes and functions that are commonly used in all analyses. The functions are\n",
    "\n",
    "- removeMissingValues()\n",
    "- removeConstantColumns()\n",
    "- runModel()\n",
    "- filterArticles()\n",
    "- createFormula()\n",
    "\n",
    "The classes are articleClass, dataContainer\n",
    "\n",
    "\"\"\"\n",
    "# from __future__ import division\n",
    "import cPickle as cp\n",
    "import pandas as pd\n",
    "#import sys\n",
    "#sys.path.append('../')    \n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf \n",
    "from scipy.stats import pearsonr, ttest_ind, ttest_rel\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from GSSUtility import *\n",
    "from cPickle import load, dump\n",
    "import random # note, scipy.random.choice doesn't work even though it ought to be the same function!!!\n",
    "import rpy2.robjects as robjects\n",
    "import pandas.rpy.common as com\n",
    "import re\n",
    "\n",
    "GSS_YEARS = [1972, 1973, 1974, 1975, 1976, 1977, 1978, \n",
    "            1980, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, \n",
    "            1990, 1991, 1993, 1994, 1996, 1998, \n",
    "            2000, 2002, 2004, 2006, 2008, 2010, 2012]\n",
    "\n",
    "\n",
    "class articleClass():\n",
    "    # attributes\n",
    "    articleID = None\n",
    "    IVs = []\n",
    "    DV = []\n",
    "    controls = []\n",
    "    centralIVs = []\n",
    "    GSSYearsUsed = []\n",
    "    GSSYearsPossible = []\n",
    "    yearPublished = None\n",
    "    missingValues = [] #? \n",
    "    '''there's a missingValues dict in the data file missingValues = {\"someNumvar1\": {\"values\": [999, -1, -2]},  # discrete values\n",
    "                 \"someNumvar2\": {\"lower\": -9, \"upper\": -1}, # range, cf. MISSING VALUES x (-9 THRU -1)\n",
    "                 \"someNumvar3\": {\"lower\": -9, \"upper\": -1, \"value\": 999},\n",
    "                 \"someStrvar1\": {\"values\": [\"foo\", \"bar\", \"baz\"]},\n",
    "                 \"someStrvar2\": {\"values': \"bletch\"}}\n",
    "                 '''\n",
    "    \n",
    "    # methods\n",
    "    def __init__(self, articleID=None, IVs=[], DVs=[], controls=[], centralIVs=[], GSSYearsUsed=[], GSSYearsPossible=[], yearPublished=None):\n",
    "        self.articleID = articleID\n",
    "        self.IVs = IVs\n",
    "        self.DVs = DVs\n",
    "        self.controls = controls\n",
    "        self.centralIVs = centralIVs\n",
    "        self.GSSYearsUsed = GSSYearsUsed\n",
    "        self.GSSYearsPossible = GSSYearsPossible\n",
    "        self.yearPublished = yearPublished\n",
    "        \n",
    "        \n",
    "class dataContainer:\n",
    "\n",
    "    # members\n",
    "    dictOfVariableGroups = []    \n",
    "    variableTypes = []\n",
    "    articleClasses = []\n",
    "    df = None\n",
    "    r = None\n",
    "    amelia = None\n",
    "    \n",
    "    def __init__(self, pathToData='../../Data/'):\n",
    "        \n",
    "        import rpy2.robjects as robjects\n",
    "        self.r = robjects.r\n",
    "        \n",
    "        from rpy2.robjects.packages import importr\n",
    "        importr('Amelia')\n",
    "        self.amelia = self.r['amelia']\n",
    "        \n",
    "        self.dictOfVariableGroups = load(open(pathToData + 'dictOfVariableGroups.pickle'))\n",
    "        self.variableTypes = load(open(pathToData + 'variableTypes.pickle'))\n",
    "        self.articleClasses = load(open(pathToData + 'articleClasses.pickle', 'rb'))\n",
    "        \n",
    "        # load the dataframe 'df' only if it hasn't been loaded yet, and put it in globals() so it's there for future runs          \n",
    "        if 'df' not in globals():\n",
    "            '''\n",
    "            pathToDf = '../../Data/GSS Dataset/stata/'\n",
    "            df = pd.read_stata(pathToDf + 'GSS7212_R4.DTA', convert_categoricals=False)\n",
    "            df.index = df['year']\n",
    "            df.columns = map(str.upper, df.columns)\n",
    "            '''\n",
    "            # going to start loading 'df' via a pickle because it's 20x faster (~2 minutes)\n",
    "            print 'Loading DataFrame df. This may take a few minutes.'            \n",
    "            pathToDf = '../../Data/'\n",
    "            import cPickle            \n",
    "#             df = cPickle.load(open(pathToDf + 'df.pickle', 'rb'))            \n",
    "            df = pd.read_pickle(pathToDf + 'df.pickle')\n",
    "            globals()['df'] = df\n",
    "            self.df = df                  \n",
    "        else: \n",
    "            self.df = globals()['df']\n",
    "            \n",
    "        \n",
    "        \n",
    "def removeMissingValues(design, axis=0):\n",
    "    '''\n",
    "    Description: Goes through each column in DataFrame and replaces its missing values with np.nan.\n",
    "    if axis=0:  gets rid of all rows that have at least one missing value.\n",
    "    if axis=1: gets rid of all columns that are entirely np.nan\n",
    "    \n",
    "    Inputs: DataFrame\n",
    "    Output: DataFrame with any rows with at least one missing value removed.\n",
    "    \n",
    "    Note: Now that I'm using the Stata version of the combined GSS data, it already has missing\n",
    "    values marked as np.nan. So, my only task is to drop those rows where this is the case. Don't \n",
    "    need to do it with this function.\n",
    "    '''\n",
    "    \n",
    "    for col in design.columns:\n",
    "        \n",
    "        mv = dataCont.MISSING_VALUES_DICT[col]\n",
    "\n",
    "        # if discrete missing values, replace them with np.nan\n",
    "        if 'values' in mv:\n",
    "            design[col].replace(mv['values'], [np.nan]*len(mv['values']), inplace=True) # it's important to have inPlace=True            \n",
    "\n",
    "        # if range of missing values [lower, upper] is given\n",
    "        elif 'lower' in mv:\n",
    "            design[col][np.array(design[col] > mv['lower']) * np.array(design[col] < mv['upper'])] = np.nan                   \n",
    "            # if there is a range, there is also always (?) a discrete value designated as missing\n",
    "            if 'value' in mv:\n",
    "                design[col].replace(mv['value'], np.nan, inplace=True) # it's important to have inPlace=True                        \n",
    "\n",
    "    if axis==0: return design.dropna(axis=0) # drop all rows with any missing values (np.nan)        \n",
    "    if axis==1: return design.dropna(axis=1, how='all')\n",
    "\n",
    "def dropRowsWithNans(dataMat, axis=0):\n",
    "    if axis==0: return dataMat.dropna(axis=0) # drop all rows with any missing values (np.nan)        \n",
    "    if axis==1: return dataMat.dropna(axis=1, how='all')\n",
    "\n",
    "\n",
    "def removeConstantColumns(design):\n",
    "    '''\n",
    "    Takes a Pandas DataFrame, searches for all columns that are constant, and drops them.\n",
    "      - if DV (first column) is constant, return None\n",
    "      - this function should be called only after all the missing value-rows are removed\n",
    "\n",
    "    input: dataframe\n",
    "    returns: dataframe without any constant columns; if DV is constant returns None\n",
    "    '''\n",
    "    if len(design.ix[:,0].unique()) == 1: return None # if DV constant\n",
    "    for col in design:\n",
    "        if len(design[col].unique()) == 1 or np.all(design[col].isnull()): # if any IVs or controls constant, drop 'em\n",
    "            print 'Dropping column', col, 'because it is constant'                    \n",
    "            #raw_input('asdfa')            \n",
    "            design = design.drop(col, axis=1) # inplace=True option not available because i'm using an old Pandas package?\n",
    "#             print design.columns\n",
    "    \n",
    "    return design\n",
    "    \n",
    "def createFormula(dataCont, design):\n",
    "    '''\n",
    "    Takes the design matrix (where first column is DV)\n",
    "    and creates a formula for Pandas/Statsmodels using the dict of variableTypes,\n",
    "    where I've coded some variables as being categorical (and specified how many levels)\n",
    "    some as continuous, and some as DONOTUSE\n",
    "    codes:\n",
    "        C = continuous, CL = continuous-like (no difference betw. this and \"C\")\n",
    "        number = categorical, where number is the number of levels\n",
    "        DONOTUSE = would need to go back to the spreadsheet file to see where I used this code (probably for things with many, many levels)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "#     print design.columns\n",
    "    \n",
    "    # LHS (left-hand side)\n",
    "    # check to make sure the DV is not 'DONOTUSE' or a categorical\n",
    "    DV = design.columns[0]\n",
    "    if DV not in dataCont.variableTypes: formula = 'standardize('+ DV +', ddof=1) ~ ' \n",
    "    else:\n",
    "        varType = dataCont.variableTypes[DV]\n",
    "        if varType == 'DONOTUSE':\n",
    "#             print 'DV %s is of type DONOTUSE' % DV\n",
    "            return None\n",
    "        elif type(varType) == int and varType > 2:\n",
    "#             print 'DV %s is categorical with more than 2 categories' % DV\n",
    "            return None\n",
    "        else:\n",
    "            formula = 'standardize('+ DV +', ddof=1) ~ ' \n",
    "\n",
    "    # RHS (right-hand side)\n",
    "    for col in design.columns[1:]: # don't include the DV in the RHS (the DV is the first element)!\n",
    " \n",
    "        if col in dataCont.variableTypes:        \n",
    "\n",
    "            varType = dataCont.variableTypes[col]        \n",
    "\n",
    "            if varType == 'DONOTUSE': \n",
    "                print 'IV %s is of type \"DONOTUSE\"' % col\n",
    "                continue\n",
    "                \n",
    "            elif type(varType) == int:\n",
    "                if varType > 15: # if >15 levels\n",
    "                    print 'categorical variable %s has more than 15 levels' % col\n",
    "                else: formula += 'C('+ col + ') + '        \n",
    "                continue\n",
    "        \n",
    "        # all other cases (not in dict, in dict but C or CL), treat it as continuous\n",
    "        formula += 'standardize('+ col + ', ddof=1) + ' # if it's not in dict, treat it as C\n",
    "    \n",
    "    # the last 3 characters should be ' + '\n",
    "    formula = formula[:-3]\n",
    "    \n",
    "#     print 'IVs count=', design.shape[1]-1, 'fomula is:', formula\n",
    "    \n",
    "    if '~' not in formula: return None # no suitable IVs added to formula\n",
    "    else: return formula\n",
    "    \n",
    "def runModel(dataCont, year, DV, IVs, controls=[]):\n",
    "    '''\n",
    "    inputs:\n",
    "      - the year of GSS to use\n",
    "      - Dependent Variable (just 1)\n",
    "      - list of independent and control variables\n",
    "      \n",
    "    outputs:\n",
    "      If OLS model estimation was possibely, return results data structure from statsmodels OLS. results contains methods like .summary() and .pvalues\n",
    "      else: return None \n",
    "    '''   \n",
    "    design = df.loc[year, [DV] + IVs + controls]\n",
    "    design = design.astype(float) # again because R messes up for ints\n",
    "    design.index = range(len(design)) # using R for imputation messes up when the index is all the same values (year)\n",
    "    \n",
    "    # gonna cut off the following from the line above.. shouldn't need it:\n",
    "    #.copy(deep=True)  # Need to make a deep copy so that original df isn't changed\n",
    "\n",
    "    # constant columns happen somewhat often, e.g. a variable like religous is always == 1 if the study also uses a varaiable\n",
    "    # like denom == the specific denomination\n",
    "    design = removeConstantColumns(design) \n",
    "\n",
    "    # create formula, and from here figure out what variables are categorical for the next step (imputation)    \n",
    "    formula = createFormula(dataCont, design)\n",
    "    if not formula: \n",
    "        print 'Couldnt construct a suitable formula'\n",
    "        return None\n",
    "    \n",
    "    # IMPUTE MISSING VALUES\n",
    "    # We will use R's \"\" module to imput missing values\n",
    "    try:\n",
    "        imputed = dataCont.amelia(com.convert_to_r_dataframe(design), m = 1, boot_type = \"none\")\n",
    "    except:\n",
    "        print 'Error during imptation. Maybe colinearity?'\n",
    "        return None\n",
    "    design = com.convert_robj(imputed.rx2('imputations').rx2('imp1'))    \n",
    "    \n",
    "    # if the line above removed DV column, then can't use this model, return None\n",
    "    if design is None or DV not in design: \n",
    "        print 'design is None or DV not in design'\n",
    "        return None    \n",
    "\n",
    "    #need to make sure there are still IVs left after we dropped some above    \n",
    "    if design.shape[1] < 2: \n",
    "        print 'no IVs available. Skipping.'\n",
    "        return None\n",
    "        \n",
    "    # skip if there's not enough data after deleting rows\n",
    "    if design.shape[0] < design.shape[1]: # if number of rows is less than number of columns\n",
    "        print 'Not enough observations. Skipping...'\n",
    "        return None\n",
    "    \n",
    "    # calculate the results   \n",
    "    try:\n",
    "        results = smf.ols(formula, data=design, missing='drop').fit() # do I need the missing='drop' part?\n",
    "    except:\n",
    "        print 'Error running model'\n",
    "        return None\n",
    "    \n",
    "    # QUALITY CHECK!!!: a check on abnormal results\n",
    "    if (abs(results.params) > 5).any() or results.rsquared > 0.98:\n",
    "        print 'Either the  params or the R^2 is too high. Skipping.'\n",
    "        return None\n",
    "        # raise <--- NEED TO THINK THROUGH WHAT TO DO HERE...\n",
    "        # Reasons this case may come up:\n",
    "        # 1. The formula has very related variables in it: 'DENOM ~ DENOM16', and correlation was 1.0                \n",
    "        # 2. The variation in DV is huge ('OTHER' [religious affiliation] or 'OCC' [occupational status]) while \n",
    "        # variation in IV is much smaller. Wait, I should standardize DV too??? Tryingt this now.\n",
    "\n",
    "    if np.isnan(results.params).any():\n",
    "        raise                \n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "'''\n",
    "description: This module contains a functil filterArticleClasses which goes through the \n",
    "  articleClasses.pickle (list of Classes) created by create_articleClasses and filters that list \n",
    "  further according to specified criteria (central variables, etc.)\n",
    "  It is to be used to set up the data, before running the actual models.\n",
    "\n",
    "returns: list of articleClasses that have passed the filters\n",
    "'''\n",
    "\n",
    "def filterArticles(articleClasses, GSSYearsUsed=True, GSSYearsPossible=False, unusedGSSYears=False, noIVs=True, noDVs=True, \\\n",
    "                    centralIVs=False, nextYearBound=0, yearPublished=False, linearModels=False, GSSCentralVariable=False):\n",
    "    '''\n",
    "    This function filters the articleClasses list according to the following criteria.\n",
    "    arguments:\n",
    "     - noIVs: skip if no IVs specified\n",
    "     - noDVs: skip if no DVs specified\n",
    "     - GSSYearsPossible: skip if there are no GSS years possible besides the ones the article used\n",
    "     - unusedGSSYears=False: If True, then keep only those articles which have some GSS Years they could have used, but didn't\n",
    "     - centralIV: skip if there is no IV(s) designated as \"central\"\n",
    "     - nextYearBound = int: skip if next future year of data is not within \"int\" of last year used\n",
    "                     = 0 by default, in which case it's not used\n",
    "     - yearPublished=False: if set to True, yearPublished is required to be not None\n",
    "     - GSSCentralVariable=False: if True, keep only those articles where GSSCentralVariable is True in the mysql\n",
    "                                 table gss_question\n",
    "     - linearModels=False: if True, keep only those articles where model type is .. and I should think about what to use here.\n",
    "     - TODO: ADD AN \"UNUSED YEARS\" filter\n",
    "    '''\n",
    "    indicesToKeep = []\n",
    "    \n",
    "    pathToData = '../../Data/'\n",
    "    if GSSCentralVariable:\n",
    "        gssCentral = cp.load(open(pathToData + 'ARTICLEID_GSS_CENTRAL_VARIABLE.pickle', 'rb'))\n",
    "\n",
    "    if linearModels:\n",
    "        modelUsed = cp.load(open(pathToData + 'ARTICLEID_AND_TRUE_IF_LINEAR_NONLINEAR.pickle', 'rb'))\n",
    "\n",
    "    for ind, a in enumerate(articleClasses):  # a = article\n",
    "        \n",
    "        # skip article if there is no info on DVs or IVs\n",
    "        # Should we change this to skip only if BOTH controls AND IVs are not there?\n",
    "        if noDVs:\n",
    "            if len(a.DVs) < 1: continue\n",
    "        \n",
    "        if noIVs: \n",
    "            if len(a.IVs) < 1: continue\n",
    "\n",
    "        if GSSYearsUsed:         \n",
    "            # if there is no used years of GSS possible to run the data on, then just skip this article\n",
    "            if len(a.GSSYearsUsed) < 1: continue\n",
    "            \n",
    "        if GSSYearsPossible:         \n",
    "            # if there is no un-used years of GSS possible to run the data on, then just skip this article\n",
    "            if len(a.GSSYearsPossible) < 1: continue\n",
    "\n",
    "        if unusedGSSYears:\n",
    "            unusedEarlyYears = [yr for yr in a.GSSYearsPossible if yr <= max(a.GSSYearsUsed)]\n",
    "            if len(unusedEarlyYears)==0: continue\n",
    "            \n",
    "        if centralIVs:    \n",
    "            # if GSS is not the central dataset used then skip\n",
    "            if len(a.centralIVs) < 1: continue\n",
    "                   \n",
    "        if nextYearBound:\n",
    "            # nextYear is an integer that specifies how soon the next available year of data is supposed to be.\n",
    "            # e.g. if nextYearBound = 4, then the new future year of data is to occur within 4 years of the last year of data\n",
    "            # actually used. \n",
    "            maxYearUsed = max(a.GSSYearsUsed)\n",
    "            futureYearsPossible = [yr for yr in a.GSSYearsPossible if yr > maxYearUsed]\n",
    "            if not futureYearsPossible or min(futureYearsPossible) > maxYearUsed + nextYearBound: continue\n",
    "                   \n",
    "        if yearPublished:\n",
    "            if not a.yearPublished: continue\n",
    "                        \n",
    "        if GSSCentralVariable:\n",
    "            if a.articleID not in gssCentral or gssCentral[a.articleID]==False: continue\n",
    "        \n",
    "        if linearModels:\n",
    "            if a.articleID not in modelUsed: continue            \n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "        # if the article survived all of the checks above add it to the list\n",
    "        indicesToKeep.append(ind)\n",
    "    \n",
    "    return [articleClasses[ind] for ind in indicesToKeep] # return elements that have survived\n",
    "                                                            # the filtering\n",
    "\n",
    "\n",
    "def identifyCognates(dataCont, LHS, cIVs, GSSYearsUsed, corrThreshold):\n",
    "    '''\n",
    "    This function takes as input the variables the articles uses on the LHS, identifies suitable\n",
    "    cognate variables and returns one of them, along with the suitable GSS years that have that cognate.\n",
    "    GSS years to use are that subset of GSSYearsUsed which also contain the cognate\n",
    "    \n",
    "    inputs:\n",
    "    \n",
    "     LHS: list of IVs and control variables\n",
    "     cIVs: list of \"central\" IVs\n",
    "     GSSYearsUsed = Years the article actually used\n",
    "     \n",
    "    returns:\n",
    "     None: if suitable cognates and years were not found\n",
    "     (cIV, cognate, GSS years to use)        \n",
    "    '''\n",
    "    # check to see if there are any cognate variables for the central IVs. if not, skip.\n",
    "    cIVsWithCognates = set(cIVs).intersection(set(dataCont.dictOfVariableGroups)) #- set(['EDUC', 'DEGREE']))        \n",
    "    if not len(cIVsWithCognates):\n",
    "        print 'No cognates for the specified central IVs'            \n",
    "        return None\n",
    "    \n",
    "    # figure out which of the central IVs actually has cognates.\n",
    "    # and choose the one that correlates most highly                \n",
    "    cIVCogPairs = {}\n",
    "            \n",
    "    for cIV in cIVsWithCognates:\n",
    "\n",
    "        potCogsMat = reduce(pd.DataFrame.append, [df.loc[yr, [cIV] + list(dataCont.dictOfVariableGroups[cIV])] for yr in GSSYearsUsed])\n",
    "\n",
    "        # some columsn will be all np.nan, because those cognates won't be in the appropriate GSS datasets\n",
    "        # get rid of those columns\n",
    "        potCogsMat = dropRowsWithNans(potCogsMat, axis=1) # this replaces ALL miss.values with np.NaN, even\n",
    "                                                           # though it only removes along axis=1\n",
    "        #  below is the version which, for a given central IV (cIV), takes the cognate that's max correlated\n",
    "        '''\n",
    "        # first value is name of variable, second is current max, third is possible years   \n",
    "        maxCorr = (None, 0.0, []) \n",
    "        for potCog in set(potCogsMat)-set([cIV]):\n",
    "            # The step below is important. I am reducing my matrix down to just two columns, cIV and potCog\n",
    "            # and removing the missing values from those (not the full matrix of cognates)            \n",
    "            subPotCogsMat = potCogsMat[[cIV, potCog]].dropna(axis=0)\n",
    "            currCorr = pearsonr(subPotCogsMat[cIV], subPotCogsMat[potCog])[0]             \n",
    "            if currCorr > maxCorr[1]:\n",
    "                # last value gives possibleYears (i.e. all unique row labels after missing values were removed)\n",
    "                maxCorr = (potCog, currCorr, subPotCogsMat.index.unique()) \n",
    "                \n",
    "        #Check to see that the potential cognate is not already in the articles' variables, \n",
    "        # and that it correlates highly enough\n",
    "        if maxCorr[0] not in LHS and maxCorr[1] > 0.5: \n",
    "            print 'Possibility:', maxCorr[0], 'in place of', cIV, '. Correlation is', maxCorr[1]\n",
    "            cIVCogPairs[cIV] = (maxCorr[0], maxCorr[2])\n",
    "        '''             \n",
    "        \n",
    "        # below is the version which for a given cIV takes a random cognate that's correlated at \n",
    "        #above some threshold amount\n",
    "        cogsPossForCIV = []        \n",
    "        for potCog in set(potCogsMat)-set([cIV]): # for each potential cognate variable       \n",
    "            subPotCogsMat = potCogsMat[[cIV, potCog]].dropna(axis=0)\n",
    "            currCorr = pearsonr(subPotCogsMat[cIV], subPotCogsMat[potCog])[0]             \n",
    "            \n",
    "            # the following line tests if the potential cognate is a) not already in the formula\n",
    "            # and b) correlated at at least corrThreshold with the cIV\n",
    "            if potCog not in LHS and currCorr > corrThreshold: \n",
    "                cogsPossForCIV.append((potCog, subPotCogsMat.index.unique()))\n",
    "        if cogsPossForCIV:\n",
    "            cIVCogPairs[cIV] = random.choice(cogsPossForCIV)\n",
    "            \n",
    "    if not cIVCogPairs:  # if there is nothing in this dict \n",
    "        print 'Could not find suitable cognate. Skipping.'\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        # of the cognate variable options, choose a random one\n",
    "        cIV, (cognate, GSSYearsWithCognate) = random.choice(cIVCogPairs.items())     \n",
    "        return cIV, cognate, GSSYearsWithCognate    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # PRACTICE ################\n",
    "\n",
    "# import rpy2.robjects as robjects\n",
    "# from rpy2.robjects import pandas2ri\n",
    "# pandas2ri.activate()\n",
    "# import pandas.rpy.common as com\n",
    "# # import GSSUtility as GU\n",
    "# from rpy2.robjects.packages import importr\n",
    "# # R's \"base\" package\n",
    "# amelia = importr('Amelia')\n",
    "\n",
    "\n",
    "# design = pd.DataFrame({'educ':np.random.randint(0,3,100), 'status':np.random.randint(0, 2, 100), 'income':np.random.randn(100)} )\n",
    "# design['tenure'] = np.random.randn(100) + 3*design.status + 2*design.educ + 3*design.income\n",
    "# design.iloc[np.random.randint(0, 100, 80), 0] = np.nan\n",
    "# design.iloc[np.random.randint(0, 100, 70), 1] = np.nan\n",
    "# design.iloc[np.random.randint(0, 100, 70), 2] = np.nan\n",
    "# design.index = range(100,200)\n",
    "\n",
    "# from rpy2.robjects import conversion\n",
    "# dataf = conversion.py2ro(design)\n",
    "\n",
    "# # amelia.amelia_amelia(design, m = 1, boot.type = \"none\")\n",
    "\n",
    "# # gonna cut off the following from the line above.. shouldn't need it:\n",
    "# #.copy(deep=True)  # Need to make a deep copy so that original df isn't changed\n",
    "\n",
    "# # constant columns happen somewhat often, e.g. a variable like religous is always == 1 if the study also uses a varaiable\n",
    "# # like denom == the specific denomination\n",
    "# # design = removeConstantColumns(design) \n",
    "\n",
    "# # IMPUTE MISSING VALUES\n",
    "# # We will use R's \"mi\" module to imput missing values\n",
    "# r = robjects.r\n",
    "# design_r_version = com.convert_to_r_dataframe(design).r_repr()\n",
    "\n",
    "# # rcode = '''\n",
    "# #     library(mi)\n",
    "# #     library(stats)\n",
    "# #     library(Amelia)\n",
    "# #     library(Zelig)\n",
    "\n",
    "# #     mydf = %s\n",
    "# #     mydfimp = amelia(mydf, m = 1, boot.type = \"none\", noms=c(\"status\", \"educ\"))\n",
    "   \n",
    "   \n",
    "# #    #mydfimp = amelia(mydf, noms=c(\"status\", \"educ\"))\n",
    "    \n",
    "# # #     res = zelig(tenure~income+factor(educ)+factor(status), data=mydfimp$imputations, model=\"ls\")\n",
    "# # #     res2 = lm(tenure~income+educ+status, data=mydf)\n",
    "    \n",
    "# # #     coefs = coef(summary(res))\n",
    "    \n",
    "# # ''' % (design_r_version)\n",
    "\n",
    "# # r(rcode)\n",
    "\n",
    "\n",
    "\n",
    "# # # # QUALITY CHECK!!!: a check on abnormal results\n",
    "# # # if (abs(results.params) > 5).any() or results.rsquared > 0.98:\n",
    "# # #     print 'Either the  params or the R^2 is too high. Skipping.'\n",
    "# # #     return None\n",
    "# # #     # raise <--- NEED TO THINK THROUGH WHAT TO DO HERE...\n",
    "# # #     # Reasons this case may come up:\n",
    "# # #     # 1. The formula has very related variables in it: 'DENOM ~ DENOM16', and correlation was 1.0                \n",
    "# # #     # 2. The variation in DV is huge ('OTHER' [religious affiliation] or 'OCC' [occupational status]) while \n",
    "# # #     # variation in IV is much smaller. Wait, I should standardize DV too??? Tryingt this now.\n",
    "\n",
    "# # # if np.isnan(results.params).any():\n",
    "# # #     raise                \n",
    "\n",
    "# # # return results\n",
    "\n",
    "# amelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# com.convert_robj(r('mydfimp$imputations$imp1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print r('summary(res)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # com.convert_robj(r('coef(summary(res))'))\n",
    "# print r('mean(as.numeric(sapply(res, summary)[\"r.squared\",]))')\n",
    "# print r('mean(as.numeric(sapply(res, summary)[\"adj.r.squared\",]))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# pattern = r'C\\(.+?\\)'\n",
    "# res = re.match(pattern, 'asdfs~C(sdfsdaf)+Casdfdsf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mydf = pd.read_csv('../Data/test_for_errors.csv', index_col=0)\n",
    "# mydf.index = range(len(mydf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from rpy2 import robjects\n",
    "# import rpy2.robjects.packages as rpackages\n",
    "# import rpy2.robjects.numpy2ri as numpy2ri\n",
    "# numpy2ri.activate()\n",
    "# # robjects.activate()\n",
    "\n",
    "\n",
    "# rpackages.importr('Amelia')\n",
    "# rpackages.importr('mi')\n",
    "\n",
    "# amelia = robjects.r['amelia']\n",
    "# mi = robjects.r['mi']\n",
    "\n",
    "# obj1 = amelia(com.convert_to_r_dataframe(mydf), m=1, boot=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# com.convert_robj(obj1.rx2('imputations').rx2('imp1'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
